{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"17_JK7ebb98bsif5l4DZgc0YFoCTD0t_m","authorship_tag":"ABX9TyOgtGqTeD5mc7llLtxq5WXl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tldextract"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6DmI5Li6kiJ","executionInfo":{"status":"ok","timestamp":1718049207150,"user_tz":-420,"elapsed":15397,"user":{"displayName":"Herwono Herwono","userId":"18260386901408372149"}},"outputId":"d610c75a-5371-492d-95fe-f61aa08eb8c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tldextract\n","  Downloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.7)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract) (2.31.0)\n","Collecting requests-file>=1.4 (from tldextract)\n","  Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract) (3.14.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract) (2024.6.2)\n","Installing collected packages: requests-file, tldextract\n","Successfully installed requests-file-2.1.0 tldextract-5.1.2\n"]}]},{"cell_type":"code","source":["# Check TLD\n","import pandas as pd\n","import tldextract\n","\n","# Read the CSV file\n","csv_file = './drive/MyDrive/Colab Notebooks/iteration-v2/dataset_url_https.csv'\n","df = pd.read_csv(csv_file)\n","\n","# Extract TLDs from URLs\n","tlds = df['url'].apply(lambda url: tldextract.extract(url).suffix)\n","\n","# Find unique TLDs\n","unique_tlds = tlds.unique()\n","\n","# Print the number of unique TLDs and the TLDs themselves\n","print(f\"Number of unique TLDs: {len(unique_tlds)}\")\n","print(\"Unique TLDs:\")\n","print(unique_tlds)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99UVU4UkMard","executionInfo":{"status":"ok","timestamp":1717923196844,"user_tz":-420,"elapsed":2721,"user":{"displayName":"Herwono Herwono","userId":"18260386901408372149"}},"outputId":"fcaa97d4-5b56-4e6d-cbd0-5c2683a93991"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique TLDs: 835\n","Unique TLDs:\n","['online' 'in' 'biz' 'ru' 'com' 'ge' 'ir' 'org' 'site' 'top' 'cloud' 'net'\n"," 'fi' 'cc' 'com.ng' 'edu.ar' 'work' 'com.tr' 'lk' 'co.mz' 'ro' 'store'\n"," 'tk' 'pro' 'sbs' 'icu' 'xyz' 'io' 'my.id' 'gr' 'rs' 'aero' 'me' 'de'\n"," 'club' 'com.br' 'shop' 'ai' 'it' 'hu' 'co.in' 'live' 'ltd' 'biz.id' 'pm'\n"," 'website' 'cz' 'cn' 'eu' 'center' 'com.au' 'org.ar' 'dev' 'fr' 'com.ar'\n"," 'com.pe' 'id' 'com.mx' 'co.nz' 'nl' 'app' 'fund' 'co.uk' 'pl' 'com.ba'\n"," 'ca' 'info' 'com.pa' 'hk' 'co.za' 'si' 'cl' 'edu.mx' 'host' 'news'\n"," 'edu.pe' 'ac.id' 'gov.ph' 'co' 'co.il' 'com.tw' 'com.co' 'net.pl' 'nu'\n"," 'es' 'ch' 'link' 'tn' 'com.lb' 'tech' 'buzz' 'tv' 'az' 'be' 'bio' 'vc'\n"," 'ae' 'pr.gov.br' 'world' 'vn' 'wf' 'ppg.br' 'com.sg' 'go.id' 'com.sv'\n"," 'pw' 'czest.pl' 'us' 'pk' 'edu.vn' 'br' 'uz' 'click' 'space' 'com.vn'\n"," 'com.pk' 'ms' 'com.bd' 'cfd' 'gd' 'one' 'se' 'net.vn' 'com.ua' 'web.id'\n"," 'blog' 'zip' 'foundation' 'my' 'sk' 'lat' 'com.cn' 'page' 'pt' 'mx'\n"," 'ac.ir' 'no' 'mw' 'vip' 'by' 'edu.mn' 'education' 'sydney' 'gob.ar' 'lt'\n"," 'kiev.ua' 'li' 'net.bd' 'gov.ua' 'ma' 'ml' 'co.th' 'pe' 'am' 'monster'\n"," 'style' 'nz' 'fun' 'gq' 'net.au' 'cam' 'mu' 'com.gt' 'or.ke' 'sch.id'\n"," 'co.ke' 'africa' 'su' 'dk' 'mn' 'com.hk' 'com.mk' 'co.tz' 'hr' 'ph'\n"," 'games' 'kz' 'gov.lr' 'fm' 'sd' 'ua' 'org.do' 'money' 'jp' 'av.tr'\n"," 'net.cn' 'school' 'express' 'coach' 'org.nz' 'name' 'gm' 'art' 'sa' 're'\n"," 'digital' 'org.br' 'ci' 'today' 'org.in' 'cf' 'com.do' 'lv' 'org.ec'\n"," 'edu' 'com.uy' 'ng' 'ac.in' 'ga' 'gt' 'tokyo' 'dz' 'or.tz' 'press' 'uk'\n"," 'gob.bo' 'solutions' 'to' 'tips' 'cm' 'college' 'gov.mk' 'net.in'\n"," 'edu.pk' 'tw' 'do' 'gob.mx' 'co.id' 'beer' 'co.jp' 'technology' 'edu.gt'\n"," 'at' 'gob.ec' 'best' 'life' 'edu.ph' 'org.np' 'edu.py' 'com.na'\n"," 'jgora.pl' 'ac.th' 'org.zm' 'design' 'gov.co' 'gob.pe' 'land' 'uy'\n"," 'org.tw' 'ba' 'coffee' 'ac.ke' 'mobi' 'biz.tr' 'gov.my' 'edu.rs' 'com.kz'\n"," 'co.zw' 'agency' 'com.ph' 'co.bw' 'futbol' 'sg' 'com.ec' 'ac.tz'\n"," 'finance' 'homes' 'rip' 'cyou' 'bond' 'quest' 'mg' 'com.jo' 'cat'\n"," 'edu.in' 'zone' 'gov.pk' 'vu' 'golf' 'ind.br' 'org.au' 'sc.gov.br'\n"," 'consulting' 'edu.sa' 'love' 'org.zw' 'edu.np' 'go.ke' 'sn' 'com.py' 'ie'\n"," 'win' 'adv.br' 'pics' 'org.pl' 'or.id' 'asn.au' 'net.ve' 'edu.co'\n"," 'edu.uy' 'beauty' 'lol' 'lu' 'name.tr' 'com.np' 'travel' 'ly' 'so' 'run'\n"," 'ee' 'cr' 'gov.zm' 'goog' 'sh' 'nf' 'gy' 'sale' 'ws' 'sld.do' 'com.my'\n"," 'LOL' 'PRO' 'ren' 'events' 'gallery' 'gov.jm' 'com.pl' 'help' 'eco'\n"," 'co.kr' 'nieruchomosci.pl' 'edu.my' 'ne.jp' 'rest' 'microsoft' 'go.th'\n"," 'io.vn' 'chat' 'support' 'cool' 'mom' 'im' 'network' 'autos' 'or.jp'\n"," 'org.bo' 'army' 'ink' 'com.sa' 'net.br' 'gg' 'navy' 'info.vn' 'pub'\n"," 'wang' 'org.cn' 'COM' 'fit' 'bar' 'gle' 'ce.gov.br' 'ht' 'storage' 'fo'\n"," 'la' 'com.ag' 'band' 'vg' 'ps' 'mv' 'net.ag' 'bid' 'fvg.it' 'mk' 'team'\n"," 'al' 'bg' 'report' 'edu.ng' 'lc' 'xn--p1ai' 'id.au' 'guru' 'com.pg'\n"," 'asia' 'com.mm' 'gl' 'ventures' 'media' 'ny.us' 'edu.lc' 'id.vn' 'md'\n"," 'com.eg' 'adm.br' 'studio' 'pro.br' 'dev.br' 'art.br' 'in.th' 'hn'\n"," 'edu.jo' 'exchange' 'men' 'kr' 'supply' 'qa' 'wiki' 'tv.br' 'cnt.br'\n"," 'capital' 'tg' 'fyi' 'warszawa.pl' 'ind.in' 'com.bo' 'ax' 'ac' 'org.mo'\n"," 'mz' 'mt' 'marketing' 'insure' 'cx' 'edu.ec' 'trade' 'gov.np' 'gs' 'ke'\n"," 'mba' 'email' 'or.th' 'tf' 'srv.br' 'tl' 'lg.jp' 'ac.uk' 'bz.it' 'gov.uk'\n"," 'gov.vn' 'gov' 'gov.in' 'gov.bd' 'ac.jp' 'org.ua' 'jaworzno.pl' 'gov.it'\n"," 'ad' 'org.uk' 'tienda' 'mat.br' 'net.ar' 'edu.ve' 'go.kr' 'pe.kr'\n"," 'elblag.pl' 'kg' 'on.ca' 'tx.us' 'go.jp' 'ac.at' 'ac.kr' 'gov.pt' 'ao'\n"," 'gov.br' 'ar' 'cern' 'edu.au' 'edu.pl' 'net.ua' 'properties' 'gov.ly'\n"," 'tn.us' 'edu.ua' 'info.pl' 'nhs.uk' 'ac.lk' 'maori.nz' 'edu.cn' 'ms.us'\n"," 'co.us' 'gal' 'or.kr' 'gov.tw' 'edu.tr' 'bt.it' 'sp.gov.br' 'ac.nz'\n"," 'mg.gov.br' 'qc.ca' 'k12.nj.us' 'abbott' 'ac.rs' 'k12.sc.us'\n"," 'himi.toyama.jp' 'nyc' 'party' 'edu.hk' 'res.in' 'bc.ca' 'is' 'genova.it'\n"," 'science' 'game.tw' 'gen.tr' 'plus' 'skin' 'miami' 'eus' 'nagoya.jp'\n"," 'edu.it' 'umbria.it' 'cv' 'td' 'org.vn' 'bz' 'in.ua' 'org.sa' 'le.it'\n"," 'gov.kz' 'int' 'gc.ca' 'reviews' 'furniture' 'org.tr' 'gob.cl' 'stream'\n"," 'gov.au' 'shizuoka.jp' 'fans' 'com.mt' 'edu.br' 'fuchu.tokyo.jp' 'org.mx'\n"," 'govt.nz' 'gr.jp' 'es.gov.br' 'com.tn' 'gov.sa' 'uji.kyoto.jp' 'au'\n"," 'nic.in' 'numazu.shizuoka.jp' 'semboku.akita.jp' 'gov.tr' 'pa.gov.br'\n"," 'ed.jp' 'CH' 'com.pt' 'gov.ar' 'waw.pl' 'shoes' 'sumy.ua' 'brussels'\n"," 'org.rs' 'amsterdam' 'sos.pl' 'ac.cn' 'lviv.ua' 'gov.cy' 'gv.at'\n"," 'miyagi.jp' 'engineering' 'gov.pl' 'pd.it' 'law' 'edu.eg' 'go.gov.br'\n"," 'kaufen' 'mil.br' 'ueda.nagano.jp' 'rv.ua' 'mil' 'campania.it' 'edu.cu'\n"," 'audi' 'gov.hk' 'vic.edu.au' 'contact' 'rocks' 'edu.tw' 'odessa.ua'\n"," 'sklep.pl' 'qld.gov.au' 'ac.za' 'calabria.it' 'tur.br' 'od.ua' 'gov.ba'\n"," 'sv.it' 'com.fr' 'wales' 'gov.by' 'lu.it' 'gujo.gifu.jp' 'ac.il' 'org.es'\n"," 'works' 'rg.it' 'gmina.pl' 'bnpparibas' 'vet' 'cs.it' 'sapporo.jp'\n"," 'k12.nc.us' 'bel.tr' 'org.za' 'ta.it' 'aq' 'gratis' 'org.il' 'directory'\n"," 'com.cy' 'md.us' 'lc.it' 'togo.aichi.jp' 'gouv.fr' 'tj' 'ce.it' 'gov.cn'\n"," 'cu' 'shiogama.miyagi.jp' 'ac.be' 'group' 'go.tz' 'ug' 'bo' 'gov.rs'\n"," 'gov.eg' 'org.hk' 'mn.us' 'domains' 'gov.lk' 'services' 'org.kw' 'co.rs'\n"," 'berlin' 'bg.it' 'dp.ua' 'palermo.it' 'edu.iq' 're.kr' 'or.us' 'co.it'\n"," 'va.it' 'cd' 'gov.ma' 'rc.it' 'gp' 'veneto.it' 'cl.it' 'org.eg' 'red'\n"," 'nagano.nagano.jp' 'pet' 'edu.bd' 'va' 'bet' 'ad.jp' 'net.co'\n"," 'nobeoka.miyazaki.jp' 'video' 'warabi.saitama.jp' 'web.tr' 'ky'\n"," 'kherson.ua' 'com.ro' 'cash' 'paris' 'com.mo' 'hants.sch.uk' 'promo'\n"," 'pr.it' 'gob.es' 'asso.fr' 'edu.kz' 'mb.ca' 'geek.nz' 'an.it'\n"," 'yamagata.gifu.jp' 'mil.pl' 'tsu.mie.jp' 'mielec.pl' 'bzh' 'com.ve'\n"," 'com.gh' 'cn.it' 'idv.tw' 'xxx' 'dn.ua' 'nb.ca' 'go.ug' 'at.it' 'ES' 'je'\n"," 'coop' 'hiroshima.jp' 'ac.gov.br' 'fish' 'expert' 'rn.gov.br' 'kh.ua'\n"," 'onl' 'toscana.it' 'taipei' 'hamburg' 'madrid' 'casino'\n"," 'sukagawa.fukushima.jp' 'nuoro.it' 'vr.it' 'fl.us' 'leg.br' 'sx.cn'\n"," 'tools' 'edu.ly' 'com.es' 'gov.py' 'vi.it' 'study' 'puglia.it'\n"," 'marche.it' 'bible' 'in.rs' 'donetsk.ua' 'opole.pl' 'org.co' 'akita.jp'\n"," 'ro.it' 'glass' 'edu.sv' 'yokohama' 'jetzt' 'il.us' 'go.cr' 'legal' 'tr'\n"," 'mo.it' 'gob.do' 'moscow' 'org.jo' 'moe' 'business' 'info.hu' 'wien'\n"," 'ac.cr' 'caltanissetta.it' 'jo' 'edu.gh' 'ag.it' 'na.it' 'tw.cn' 'com.am'\n"," 'chuo.yamanashi.jp' 'k12.tr' 'gov.lb' 'name.vn' 'rj.gov.br' 'mi.it'\n"," 'edu.pt' 'org.gt' 'webcam' 'schwarz' 'katowice.pl' 'k12.ca.us'\n"," 'olecko.pl' 'tottori.jp' 'nc' 'soy' 'tn.it' 'dog' 's-lanark.sch.uk'\n"," 'gov.mo' 'rm.it' 'bialystok.pl' 'university' 'gov.iq' 'fi.it'\n"," 'herts.sch.uk' 'pb.gov.br' 'ma.gov.br' 'org.al' 'net.tw' 'pe.gov.br'\n"," 'agric.za' 'academy' 'zt.ua' 'equipment' 'co.at' 'sutton.sch.uk' 'org.bw'\n"," 'ki' 'careers' 'edu.hn' 'eco.br' 'ck.ua' 'oh.us' 'sy' 'if.ua' 'varese.it'\n"," 'google' 'al.us' 'gov.za' 'org.pe' 'ms.it' 'rs.gov.br' 'morioka.iwate.jp'\n"," 'market' 'ist' 'seto.aichi.jp' 'gov.jo' 'com.gr' 'ovh' 'org.bs' 'vt.us'\n"," 'radom.pl' 'ac.pa' 'makeup' 'kitchen' 'gov.bh' 'edu.ba' 'sk.ca' 'family'\n"," 'company' 'bm' 'wtf' 'show' 'gov.sg' 'solar' 'dentist' 'software'\n"," 'roma.it' 'farm' 'k12.in.us' 'limo']\n"]}]},{"cell_type":"code","source":["# Certificate Features (it has duplicate issue)\n","import os\n","import json\n","from datetime import datetime\n","import pandas as pd\n","from urllib.parse import urlparse\n","import category_encoders as ce\n","from tqdm import tqdm\n","\n","# Directory containing JSON files\n","directory = \"/Users/herwonowr/Downloads/output\"\n","\n","# Load URLs and labels from dataset_url_https.csv into a DataFrame\n","dataset_path = \"/Users/herwonowr/Downloads/dataset_url_https.csv\"\n","dataset_df = pd.read_csv(dataset_path)\n","pd.set_option('future.no_silent_downcasting', True)\n","\n","# For the testing, only load sample data\n","dataset_df = dataset_df.sample(n=500, random_state=1)\n","\n","# Extract domains from URLs and create a dictionary for quick lookups\n","# url_domain_map = {urlparse(url).netloc: (url, label) for url, label in zip(dataset_df['url'], dataset_df['label'])}\n","# domains_set = set(urlparse(url).netloc for url in dataset_df['url'])\n","\n","url_domain_map = {urlparse(url).hostname: (url, label) for url, label in zip(dataset_df['url'], dataset_df['label'])}\n","domains_set = set(urlparse(url).hostname for url in dataset_df['url'])\n","\n","# Initialize list to hold data\n","features_list = []\n","certificates_list = []\n","\n","# Process all domains in the dataset\n","for domain in tqdm(domains_set, desc=\"Processing domains\"):\n","    json_filename = f\"{domain}.json\"\n","    if json_filename in os.listdir(directory):\n","        with open(os.path.join(directory, json_filename), 'r') as file:\n","            data = json.load(file)\n","            if not data:  # Check if the data is empty\n","                # Find the original URL and label\n","                original_url, label = url_domain_map[domain]\n","\n","                # Default features for empty JSON files\n","                features = {\n","                    \"url\": original_url,\n","                    \"domain\": domain,\n","                    \"label\": label,\n","                    \"tls_lifetime\": 0,\n","                    \"tls_average_inter_arrival_time\": 0,\n","                    \"tls_number_of_certificates\": 0,\n","                    \"tls_average_san_list_size\": 0,\n","                    \"tls_average_validity_period\": 0,\n","                }\n","\n","                # Append the features to the list\n","                features_list.append(features)\n","            else:\n","                for entry in data:\n","                    entry['domain'] = domain\n","                    certificates_list.append(entry)\n","\n","# Create DataFrame from certificates\n","df = pd.DataFrame(certificates_list)\n","\n","# Initialize binary encoders for common_names and issuers\n","encoder_common_names = ce.BinaryEncoder(cols=['common_name'])\n","encoder_issuers = ce.BinaryEncoder(cols=['issuer_name'])\n","\n","# Fit the encoders on the full dataset to determine the maximum number of binary columns\n","encoded_common_names_df = encoder_common_names.fit_transform(df['common_name'])\n","encoded_issuers_df = encoder_issuers.fit_transform(df['issuer_name'])\n","max_common_names_columns = encoded_common_names_df.shape[1]\n","max_issuers_columns = encoded_issuers_df.shape[1]\n","\n","# Rename columns to match the expected format\n","encoded_common_names_df.columns = [f'tls_common_names_{i}' for i in range(max_common_names_columns)]\n","encoded_issuers_df.columns = [f'tls_issuers_{i}' for i in range(max_issuers_columns)]\n","\n","# Process each domain to extract features\n","for domain in tqdm(domains_set, desc=\"Extracting features\"):\n","    if domain in df['domain'].unique():\n","        domain_certs = df[df['domain'] == domain]\n","\n","        if not domain_certs.empty:\n","            domain_certs = domain_certs.sort_values(by=\"not_before\")\n","\n","            first_cert_issue_date = datetime.fromisoformat(domain_certs.iloc[0][\"not_before\"])\n","            last_cert_expire_date = datetime.fromisoformat(domain_certs.iloc[-1][\"not_after\"])\n","\n","            lifetime = (last_cert_expire_date - first_cert_issue_date).days\n","\n","            inter_arrival_times = []\n","            for i in range(1, len(domain_certs)):\n","                previous_issue_date = datetime.fromisoformat(domain_certs.iloc[i-1][\"not_before\"])\n","                current_issue_date = datetime.fromisoformat(domain_certs.iloc[i][\"not_before\"])\n","                inter_arrival_times.append((current_issue_date - previous_issue_date).days)\n","\n","            average_inter_arrival_time = sum(inter_arrival_times) / len(inter_arrival_times) if inter_arrival_times else 0\n","\n","            number_of_certificates = len(domain_certs)\n","\n","            san_list_sizes = [len(cert.split(\"\\n\")) for cert in domain_certs[\"name_value\"]]\n","            average_san_list_size = sum(san_list_sizes) / len(san_list_sizes)\n","\n","            issuers = list(set(domain_certs[\"issuer_name\"].tolist()))\n","            common_names = list(set(domain_certs[\"common_name\"].tolist()))\n","\n","            # Perform binary encoding for common_names and issuers\n","            encoded_common_names = encoder_common_names.transform(pd.DataFrame(common_names, columns=['common_name']))\n","            encoded_issuers = encoder_issuers.transform(pd.DataFrame(issuers, columns=['issuer_name']))\n","\n","            # Rename column\n","            encoded_common_names.columns = [f'tls_common_names_{i}' for i in range(max_common_names_columns)]\n","            encoded_issuers.columns = [f'tls_issuers_{i}' for i in range(max_issuers_columns)]\n","\n","            # Sum the binary encoded values\n","            encoded_common_names_sum = encoded_common_names.sum().to_dict()\n","            encoded_issuers_sum = encoded_issuers.sum().to_dict()\n","\n","            validity_periods = []\n","            for not_before, not_after in zip(domain_certs[\"not_before\"], domain_certs[\"not_after\"]):\n","                not_before_date = datetime.fromisoformat(not_before)\n","                not_after_date = datetime.fromisoformat(not_after)\n","                validity_periods.append((not_after_date - not_before_date).days)\n","\n","            average_validity_period = sum(validity_periods) / len(validity_periods)\n","\n","            original_url, label = url_domain_map[domain]\n","            features = {\n","                \"url\": original_url,\n","                \"domain\": domain,\n","                \"label\": label,\n","                \"tls_lifetime\": lifetime,\n","                \"tls_average_inter_arrival_time\": average_inter_arrival_time,\n","                \"tls_number_of_certificates\": number_of_certificates,\n","                \"tls_average_san_list_size\": average_san_list_size,\n","                \"tls_average_validity_period\": average_validity_period,\n","            }\n","\n","            # Update features with encoded sums\n","            features.update(encoded_common_names_sum)\n","            features.update(encoded_issuers_sum)\n","\n","            # Append the features to the list\n","            features_list.append(features)\n","    else:\n","        # Handle case where there are no certificates for a domain\n","        original_url, label = url_domain_map[domain]\n","        features = {\n","            \"url\": original_url,\n","            \"domain\": domain,\n","            \"label\": label,\n","            \"tls_lifetime\": 0,\n","            \"tls_average_inter_arrival_time\": 0,\n","            \"tls_number_of_certificates\": 0,\n","            \"tls_average_san_list_size\": 0,\n","            \"tls_average_validity_period\": 0,\n","        }\n","\n","        features_list.append(features)\n","\n","# Fill NaN tls_common_names and tls_issuers binary encoding\n","for feature in features_list:\n","    for i in range(max_common_names_columns):\n","        col_name = f'tls_common_names_{i}'\n","        if col_name not in feature:\n","            feature[col_name] = 0.0\n","    for i in range(max_issuers_columns):\n","        col_name = f'tls_issuers_{i}'\n","        if col_name not in feature:\n","            feature[col_name] = 0.0\n","\n","# Create DataFrame for features\n","features_df = pd.DataFrame(features_list)\n","\n","# Save new dataset\n","features_df.to_csv('dataset_url_https_tls.csv', index=False)\n","\n","#  'tls_lifetime',\n","#  'tls_average_inter_arrival_time',\n","#  'tls_number_of_certificates',\n","#  'tls_average_san_list_size',\n","#  'tls_average_validity_period',\n","#  'tls_number_common_names_distinct',\n","#  'tls_number_issuers_distinct',"],"metadata":{"id":"hAZMAN8p5rAs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fix duplicate and index issue in Certificate features extraction\n","import pandas as pd\n","\n","# Load the dataset\n","dataset_path = \"./drive/MyDrive/Colab Notebooks/iteration-v2/dataset_url_https_tls.csv\"\n","df = pd.read_csv(dataset_path)\n","pd.set_option('future.no_silent_downcasting', True)\n","\n","# Drop the 'idx' column\n","df = df.drop(columns=['idx'])\n","\n","# Drop duplicate rows\n","df_cleaned = df.drop_duplicates()\n","\n","# Save the cleaned dataset to a new file\n","cleaned_dataset_path = \"./drive/MyDrive/Colab Notebooks/iteration-v2/cleaned_dataset_url_https_tls.csv\"\n","df_cleaned.to_csv(cleaned_dataset_path, index=False)\n","\n","print(f\"Cleaned dataset saved to {cleaned_dataset_path}\")"],"metadata":{"id":"kyfOFcEclIJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Lexical Features v2\n","import pandas as pd\n","import urllib.parse\n","import re\n","import math\n","from collections import Counter\n","import tldextract\n","import idna\n","\n","# Function to calculate Shannon entropy\n","def shannon_entropy(data):\n","    if not data:\n","        return 0\n","    entropy = 0\n","    counter = Counter(data)\n","    length = len(data)\n","    for count in counter.values():\n","        p = count / length\n","        entropy -= p * math.log2(p)\n","    return entropy\n","\n","# Function to extract features from a hostname\n","def extract_hostname_features(url, hostname):\n","    # Check if the hostname needs Punycode encoding\n","    try:\n","        punycode_hostname = idna.encode(hostname).decode('ascii')\n","        uses_punycode = 1 if punycode_hostname != hostname else 0\n","    except idna.IDNAError:\n","        punycode_hostname = hostname\n","        uses_punycode = 0\n","\n","    extracted = tldextract.extract(punycode_hostname)\n","    domain = f\"{extracted.domain}.{extracted.suffix}\"\n","    tld = extracted.suffix\n","    subdomains = extracted.subdomain.split('.') if extracted.subdomain else []\n","    path_segments = punycode_hostname.split('.')\n","\n","    # Transform list features into scalar features\n","    number_of_digits_in_subdomains = [sum(1 for char in sub if char.isdigit()) for sub in subdomains]\n","    number_of_alphabetic_in_subdomains = [sum(1 for char in sub if char.isalpha()) for sub in subdomains]\n","    number_of_hyphens_in_subdomains = [sub.count('-') for sub in subdomains]\n","    vowel_to_consonant_ratio_in_subdomains = [\n","        sum(1 for char in sub if char in 'aeiouAEIOU') / (sum(1 for char in sub if char.isalpha() and char not in 'aeiouAEIOU') or 1) for sub in subdomains\n","    ]\n","    entropy_of_each_subdomain = [shannon_entropy(sub) for sub in subdomains]\n","\n","    features = {\n","        # Domain related features\n","        \"url\": url,\n","        \"domain\": punycode_hostname,\n","        \"length_of_domain\": len(domain),\n","        \"number_of_subdomains\": len(subdomains),\n","        \"number_of_dots\": punycode_hostname.count('.'),\n","        \"tld\": tld,\n","\n","        # Character composition features\n","        \"number_of_digits\": sum(1 for char in punycode_hostname if char.isdigit()),\n","        \"number_of_special_characters\": sum(1 for char in punycode_hostname if char == '-'),\n","        \"number_of_alphabetic_sequences\": len(re.findall(r'[a-zA-Z]+', punycode_hostname)),\n","        \"number_of_vowels\": sum(1 for char in punycode_hostname if char in 'aeiouAEIOU'),\n","        \"number_of_consonants\": sum(1 for char in punycode_hostname if char.isalpha() and char not in 'aeiouAEIOU'),\n","\n","        # Density and ratio features\n","        \"vowel_to_consonant_ratio\": sum(1 for char in punycode_hostname if char in 'aeiouAEIOU') / (sum(1 for char in punycode_hostname if char.isalpha() and char not in 'aeiouAEIOU') or 1),\n","        \"vowel_density\": sum(1 for char in punycode_hostname if char in 'aeiouAEIOU') / len(punycode_hostname),\n","        \"consonant_density\": sum(1 for char in punycode_hostname if char.isalpha() and char not in 'aeiouAEIOU') / len(punycode_hostname),\n","        \"digit_density\": sum(1 for char in punycode_hostname if char.isdigit()) / len(punycode_hostname),\n","\n","        # Shannon entropy features\n","        \"hostname_entropy\": shannon_entropy(punycode_hostname),\n","        \"shannon_entropy_of_vowels\": shannon_entropy([char for char in punycode_hostname if char in 'aeiouAEIOU']),\n","        \"shannon_entropy_of_consonants\": shannon_entropy([char for char in punycode_hostname if char.isalpha() and char not in 'aeiouAEIOU']),\n","        \"shannon_entropy_of_digits\": shannon_entropy([char for char in punycode_hostname if char.isdigit()]),\n","\n","        # Path segment features\n","        \"total_number_of_tokens\": len(path_segments),\n","        \"maximum_token_length\": max((len(segment) for segment in path_segments), default=0),\n","        \"minimum_token_length\": min((len(segment) for segment in path_segments), default=0),\n","        \"average_token_length\": sum(len(segment) for segment in path_segments) / (len(path_segments) or 1),\n","        \"token_length_variance\": (lambda lengths: sum((x - sum(lengths) / len(lengths)) ** 2 for x in lengths) / len(lengths))(list(map(len, path_segments))) if path_segments else 0,\n","        \"mean_token_length\": sum(len(segment) for segment in path_segments) / (len(path_segments) or 1),\n","        \"standard_deviation_of_token_length\": (lambda lengths: math.sqrt(sum((x - sum(lengths) / len(lengths)) ** 2 for x in lengths) / len(lengths)))(list(map(len, path_segments))) if path_segments else 0,\n","\n","        # Additional lexical features\n","        \"ratio_of_meaningful_syllables\": sum(1 for segment in path_segments for char in segment if char in 'aeiouAEIOU') / (sum(1 for segment in path_segments for char in segment if char.isalpha()) or 1),\n","        \"lexical_density\": sum(1 for segment in path_segments if segment.isalpha()) / (len(path_segments) or 1),\n","\n","        # Longest sequences\n","        \"longest_alphabetic_sequence\": max((len(seq) for seq in re.findall(r'[a-zA-Z]+', punycode_hostname)), default=0),\n","        \"longest_digit_sequence\": max((len(seq) for seq in re.findall(r'\\d+', punycode_hostname)), default=0),\n","\n","        # Transformed list features\n","        \"sum_digits_in_subdomains\": sum(number_of_digits_in_subdomains),\n","        \"mean_digits_in_subdomains\": sum(number_of_digits_in_subdomains) / (len(number_of_digits_in_subdomains) or 1),\n","        \"max_digits_in_subdomains\": max(number_of_digits_in_subdomains, default=0),\n","        \"min_digits_in_subdomains\": min(number_of_digits_in_subdomains, default=0),\n","\n","        \"sum_alphabetic_in_subdomains\": sum(number_of_alphabetic_in_subdomains),\n","        \"mean_alphabetic_in_subdomains\": sum(number_of_alphabetic_in_subdomains) / (len(number_of_alphabetic_in_subdomains) or 1),\n","        \"max_alphabetic_in_subdomains\": max(number_of_alphabetic_in_subdomains, default=0),\n","        \"min_alphabetic_in_subdomains\": min(number_of_alphabetic_in_subdomains, default=0),\n","\n","        \"sum_hyphens_in_subdomains\": sum(number_of_hyphens_in_subdomains),\n","        \"mean_hyphens_in_subdomains\": sum(number_of_hyphens_in_subdomains) / (len(number_of_hyphens_in_subdomains) or 1),\n","        \"max_hyphens_in_subdomains\": max(number_of_hyphens_in_subdomains, default=0),\n","        \"min_hyphens_in_subdomains\": min(number_of_hyphens_in_subdomains, default=0),\n","\n","        \"sum_vowel_to_consonant_ratio_in_subdomains\": sum(vowel_to_consonant_ratio_in_subdomains),\n","        \"mean_vowel_to_consonant_ratio_in_subdomains\": sum(vowel_to_consonant_ratio_in_subdomains) / (len(vowel_to_consonant_ratio_in_subdomains) or 1),\n","        \"max_vowel_to_consonant_ratio_in_subdomains\": max(vowel_to_consonant_ratio_in_subdomains, default=0),\n","        \"min_vowel_to_consonant_ratio_in_subdomains\": min(vowel_to_consonant_ratio_in_subdomains, default=0),\n","\n","        \"sum_entropy_of_subdomains\": sum(entropy_of_each_subdomain),\n","        \"mean_entropy_of_subdomains\": sum(entropy_of_each_subdomain) / (len(entropy_of_each_subdomain) or 1),\n","        \"max_entropy_of_subdomains\": max(entropy_of_each_subdomain, default=0),\n","        \"min_entropy_of_subdomains\": min(entropy_of_each_subdomain, default=0),\n","\n","        # Additional pattern checks (possibly not used)\n","        # \"uses_punycode\": uses_punycode\n","    }\n","\n","    return features\n","\n","# Read the CSV file\n","csv_file = './drive/MyDrive/Colab Notebooks/iteration-v2/cleaned_dataset_url_https_tls.csv'\n","df = pd.read_csv(csv_file)\n","pd.set_option('future.no_silent_downcasting', True)\n","\n","# Add the test domain to the DataFrame\n","# test_row = pd.DataFrame([{'url': 'https://스타벅스코리아.com', 'label': 1}])\n","# df = pd.concat([df, test_row], ignore_index=True)\n","\n","# Extract TLDs from URLs\n","df['tld'] = df['url'].apply(lambda url: tldextract.extract(url).suffix)\n","\n","# Calculate TLD frequency (frequency encoding)\n","tld_frequency = df['tld'].value_counts().to_dict()\n","\n","# Map TLD to its frequency\n","df['tld_frequency'] = df['tld'].map(tld_frequency)\n","\n","# Extract features for each URL\n","features_list = []\n","for index, row in df.iterrows():\n","    url = row['url']\n","    # label = row['label']\n","    parsed_url = urllib.parse.urlparse(url)\n","    hostname = parsed_url.hostname\n","    if hostname:\n","        features = extract_hostname_features(url, hostname)\n","        features['tld_frequency'] = row['tld_frequency']\n","        # features['label'] = label\n","        features_list.append(features)\n","\n","# Create a dataframe from the features\n","features_df = pd.DataFrame(features_list)\n","\n","# Merge with the original dataframe, dropping any existing 'tld' and 'tld_frequency' columns\n","df.drop(columns=['tld', 'tld_frequency'], inplace=True, errors='ignore')\n","merged_df = df.merge(features_df.drop(columns=['domain']), on='url', how='left')\n","\n","# Define the order of columns as desired\n","desired_order = ['url', 'domain', 'tld', 'label'] + [col for col in features_df.columns if col not in ['url', 'domain', 'tld']] + [col for col in df.columns if col not in ['url', 'domain', 'tld', 'label']]\n","\n","# Reorder the dataframe\n","final_df = merged_df[desired_order]\n","\n","# Save the final features DataFrame to a new CSV file\n","final_df.to_csv('./drive/MyDrive/Colab Notebooks/iteration-v2/dataset_url_https_tls_lexical_features.csv', index=False)\n","\n","print(f\"Total dataset: {final_df.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vtzDFMr6scH0","executionInfo":{"status":"ok","timestamp":1718050746442,"user_tz":-420,"elapsed":30526,"user":{"displayName":"Herwono Herwono","userId":"18260386901408372149"}},"outputId":"7d7f6f0e-72d2-438d-ce00-c6d5554eadec"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total dataset: (64322, 85)\n"]}]},{"cell_type":"code","source":["# Extract PDNS\n","import pandas as pd\n","import os\n","import json\n","\n","# Load the CSV file and extract the 'domain' column\n","csv_file_path = './drive/MyDrive/Colab Notebooks/iteration-v2/dataset_url_https_tls_lexical_features.csv'\n","df = pd.read_csv(csv_file_path)\n","domain_set = set(df['domain'].tolist())\n","\n","# Check the number of data in the 'domain' set\n","num_domains = len(domain_set)\n","print(f\"Number of domains: {num_domains}\")\n","\n","# Initialize the list to hold the final data\n","final_data = []\n","\n","# Folder containing JSON files\n","json_folder_path = 'pdns'\n","\n","# Set to track domains found in JSON files\n","json_domain_set = set()\n","\n","# Process each JSON file in the folder\n","for json_file in os.listdir(json_folder_path):\n","    if json_file.endswith('.json'):\n","        domain = json_file.replace('.json', '')\n","        json_domain_set.add(domain)\n","        with open(os.path.join(json_folder_path, json_file), 'r') as file:\n","            json_content = json.load(file)\n","            # Create a dictionary for each domain with its JSON content\n","            data_entry = {'domain': domain}\n","            data_entry.update(json_content)\n","            final_data.append(data_entry)\n","\n","# Add domains not found in JSON files with all features set to 0\n","default_features = {\n","    \"pdns_record_age\": 0,\n","    \"pdns_number_of_ip\": 0,\n","    \"pdns_change_frequency\": 0,\n","    \"pdns_unique_rrtype\": 0,\n","    \"pdns_unique_rrdata\": 0\n","}\n","\n","for domain in domain_set - json_domain_set:\n","    data_entry = {'domain': domain}\n","    data_entry.update(default_features)\n","    final_data.append(data_entry)\n","\n","# Create a DataFrame from the final data and save to a new CSV file\n","final_df = pd.DataFrame(final_data)\n","output_csv_path = './drive/MyDrive/Colab Notebooks/iteration-v2/dataset_pdns_features.csv'\n","final_df.to_csv(output_csv_path, index=False)\n","\n","print(f'Total rows: {final_df.shape[0]}')\n","print(f\"Final data saved to {output_csv_path}\")\n"],"metadata":{"id":"uUbv2HT-LyYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Merge PDNS Features\n","import pandas as pd\n","\n","# Load the original dataset\n","original_csv_path = './drive/MyDrive/Colab Notebooks/iteration-v2/dataset_url_https_tls_lexical_features.csv'\n","original_df = pd.read_csv(original_csv_path)\n","\n","# Load the domain PDNS features dataset\n","pdns_csv_path = './drive/MyDrive/Colab Notebooks/iteration-v2/dataset_pdns_features.csv'\n","pdns_df = pd.read_csv(pdns_csv_path)\n","\n","# Merge the two dataframes on the 'domain' column\n","combined_df = pd.merge(original_df, pdns_df, on='domain', how='left')\n","\n","# Ensure the column order: all columns from original_df followed by columns from pdns_df (excluding 'domain' from pdns_df)\n","original_columns = list(original_df.columns)\n","pdns_columns = [col for col in pdns_df.columns if col != 'domain']\n","combined_columns = original_columns + pdns_columns\n","combined_df = combined_df[combined_columns]\n","\n","# Save the combined DataFrame to a new CSV file\n","output_csv_path = './drive/MyDrive/Colab Notebooks/iteration-v2/dataset_url_https_tls_lexical_pdns_features.csv'\n","combined_df.to_csv(output_csv_path, index=False)\n","\n","print(f\"Combined data saved to {output_csv_path}\")\n","\n","duplicates = combined_df.duplicated()\n","if duplicates.any():\n","    print(f\"Warning: There are {duplicates.sum()} duplicate rows in the combined dataset.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rnjg6TkNL2AL","executionInfo":{"status":"ok","timestamp":1718477137152,"user_tz":-420,"elapsed":7070,"user":{"displayName":"Herwono Herwono","userId":"18260386901408372149"}},"outputId":"17c06447-9487-456c-a505-26ed665b03ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Combined data saved to ./drive/MyDrive/Colab Notebooks/iteration-v2/dataset_url_https_tls_lexical_pdns_features.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","csv_file = './drive/MyDrive/Colab Notebooks/iteration-v2/v2_dataset_url_https_tls_lexical_pdns_features.csv'\n","df = pd.read_csv(csv_file)\n","# df.shape[0]\n","df.columns.tolist()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OMPPf5wb2xZ3","executionInfo":{"status":"ok","timestamp":1724666462034,"user_tz":-420,"elapsed":2064,"user":{"displayName":"Herwono Herwono","userId":"18260386901408372149"}},"outputId":"0e435659-9dcc-4e24-d24b-15ca105ccd0c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['url',\n"," 'domain',\n"," 'tld',\n"," 'label',\n"," 'length_of_domain',\n"," 'number_of_subdomains',\n"," 'number_of_dots',\n"," 'number_of_digits',\n"," 'number_of_special_characters',\n"," 'number_of_alphabetic_sequences',\n"," 'number_of_vowels',\n"," 'number_of_consonants',\n"," 'vowel_to_consonant_ratio',\n"," 'vowel_density',\n"," 'consonant_density',\n"," 'digit_density',\n"," 'hostname_entropy',\n"," 'shannon_entropy_of_vowels',\n"," 'shannon_entropy_of_consonants',\n"," 'shannon_entropy_of_digits',\n"," 'total_number_of_tokens',\n"," 'maximum_token_length',\n"," 'minimum_token_length',\n"," 'average_token_length',\n"," 'token_length_variance',\n"," 'mean_token_length',\n"," 'standard_deviation_of_token_length',\n"," 'ratio_of_meaningful_syllables',\n"," 'lexical_density',\n"," 'longest_alphabetic_sequence',\n"," 'longest_digit_sequence',\n"," 'sum_digits_in_subdomains',\n"," 'mean_digits_in_subdomains',\n"," 'max_digits_in_subdomains',\n"," 'min_digits_in_subdomains',\n"," 'sum_alphabetic_in_subdomains',\n"," 'mean_alphabetic_in_subdomains',\n"," 'max_alphabetic_in_subdomains',\n"," 'min_alphabetic_in_subdomains',\n"," 'sum_hyphens_in_subdomains',\n"," 'mean_hyphens_in_subdomains',\n"," 'max_hyphens_in_subdomains',\n"," 'min_hyphens_in_subdomains',\n"," 'sum_vowel_to_consonant_ratio_in_subdomains',\n"," 'mean_vowel_to_consonant_ratio_in_subdomains',\n"," 'max_vowel_to_consonant_ratio_in_subdomains',\n"," 'min_vowel_to_consonant_ratio_in_subdomains',\n"," 'sum_entropy_of_subdomains',\n"," 'mean_entropy_of_subdomains',\n"," 'max_entropy_of_subdomains',\n"," 'min_entropy_of_subdomains',\n"," 'tld_frequency',\n"," 'tls_lifetime',\n"," 'tls_average_inter_arrival_time',\n"," 'tls_number_of_certificates',\n"," 'tls_average_san_list_size',\n"," 'tls_average_validity_period',\n"," 'tls_number_common_names_distinct',\n"," 'tls_number_issuers_distinct',\n"," 'pdns_record_age',\n"," 'pdns_number_of_ip',\n"," 'pdns_change_frequency',\n"," 'pdns_unique_rrtype',\n"," 'pdns_unique_rrdata']"]},"metadata":{},"execution_count":3}]}]}